{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4d3e9b6b-778b-4771-b262-558bcc337844",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "If you have any questions about the code, please contact: anjialin8@gmail.com                           \n",
    "                                                            \n",
    "In this article, the dataset is divided into two parts: training and testing. Please contact the author to download the dataset.                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "539a3689-7bf6-4551-9929-796062734183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from thop import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21714fd-ea5c-47c6-be03-711d8a7da74c",
   "metadata": {},
   "source": [
    "# 读取数据模块\n",
    "## 加载训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c11c72a-0ebb-4164-9ae9-07160bb33cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1290])\n",
      "torch.Size([1290, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "FILE_TRAIN_DATA = \"dataset/train/data\"\n",
    "FILE_TRAIN_LABEL = \"dataset/train/label\"\n",
    "files_train_data = os.listdir(FILE_TRAIN_DATA)\n",
    "files_train_label = os.listdir(FILE_TRAIN_LABEL)\n",
    "vaule, labels = [], []\n",
    "for file in files_train_data:\n",
    "    FILE_DATA = os.path.join(FILE_TRAIN_DATA, file)\n",
    "    for file_data in os.listdir(FILE_DATA):\n",
    "        image = numpy.array(Image.open(os.path.join(FILE_DATA, file_data)))\n",
    "        image = numpy.transpose(image, (2, 0, 1))\n",
    "        time_match = re.search(r'\\d+', file_data)\n",
    "        time_value = int(time_match.group()) if time_match else 0\n",
    "        time_factor = time_value / 1000.0\n",
    "        time_matrix = numpy.ones((3, 256, 256), dtype=numpy.float32) * time_factor\n",
    "        image = image + time_matrix\n",
    "        vaule.append(image)\n",
    "for file in files_train_label:\n",
    "    FILE_LABEL = numpy.load(os.path.join(FILE_TRAIN_LABEL, file))\n",
    "    for file_data in FILE_LABEL:\n",
    "        labels.append(file_data)\n",
    "LABELS_TRAIN = torch.as_tensor(numpy.array(labels), dtype = torch.float32)\n",
    "VAULE_TRAIN = torch.as_tensor(numpy.array(vaule), dtype = torch.float32)\n",
    "print(LABELS_TRAIN.shape)\n",
    "print(VAULE_TRAIN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bfbc9-cad9-4de0-90c3-7f3f0cb87f1d",
   "metadata": {},
   "source": [
    "## 加载测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a6d7e72-b477-472b-a8f5-c9d59b4ef79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1239])\n",
      "torch.Size([1239, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "FILE_TEST_DATA = \"dataset/test/data\" \n",
    "FILE_TEST_LABEL = \"dataset/test/label\"\n",
    "files_test_vaule = os.listdir(FILE_TEST_DATA)\n",
    "files_test_label = os.listdir(FILE_TEST_LABEL)\n",
    "vaule, labels = [], []\n",
    "for file in files_test_vaule:\n",
    "    FILE_DATA = os.path.join(FILE_TEST_DATA, file)\n",
    "    for file_data in os.listdir(FILE_DATA):\n",
    "        image = numpy.array(Image.open(os.path.join(FILE_DATA, file_data)))\n",
    "        image = numpy.transpose(image, (2, 0, 1))\n",
    "        time_match = re.search(r'\\d+', file_data)\n",
    "        time_value = int(time_match.group()) if time_match else 0\n",
    "        time_factor = time_value / 1000.0\n",
    "        time_matrix = numpy.ones((3, 256, 256), dtype=numpy.float32) * time_factor\n",
    "        image = image + time_matrix\n",
    "        vaule.append(image)\n",
    "for file in files_test_label:\n",
    "    FILE_LABEL = numpy.load(os.path.join(FILE_TEST_LABEL, file))\n",
    "    for file_data in FILE_LABEL:\n",
    "        labels.append(file_data)\n",
    "LABELS_TEST = torch.as_tensor(numpy.array(labels), dtype = torch.float32)\n",
    "VAULE_TEST = torch.as_tensor(numpy.array(vaule), dtype = torch.float32)\n",
    "print(LABELS_TEST.shape)\n",
    "print(VAULE_TEST.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212bc631-0d69-4466-b15d-c4c51bbecefb",
   "metadata": {},
   "source": [
    "## 打乱数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60400927-faf8-47b6-9a6e-4fd2598724df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shuffled successfully!\n",
      "Data shuffled successfully!\n"
     ]
    }
   ],
   "source": [
    "My_Dataset = torch.utils.data.TensorDataset(VAULE_TRAIN, LABELS_TRAIN)\n",
    "train, val = torch.utils.data.random_split(dataset=My_Dataset, lengths=[round(0.9*VAULE_TRAIN.shape[0]), round(0.1*VAULE_TRAIN.shape[0])], generator=torch.Generator().manual_seed(0))\n",
    "train = torch.utils.data.DataLoader(train, batch_size =32, shuffle = True)\n",
    "val = torch.utils.data.DataLoader(val, batch_size = 32, shuffle = True)\n",
    "print(\"Data shuffled successfully!\")\n",
    "\n",
    "My_Dataset = torch.utils.data.TensorDataset(VAULE_TEST, LABELS_TEST)\n",
    "test, _ = torch.utils.data.random_split(dataset=My_Dataset, lengths=[round(0.9*VAULE_TEST.shape[0]), round(0.1*VAULE_TEST.shape[0])], generator=torch.Generator().manual_seed(0))\n",
    "test = torch.utils.data.DataLoader(test, batch_size =32, shuffle = True)\n",
    "print(\"Data shuffled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907222fa-d954-4ff6-b3c8-fd53d0bf6533",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2436a118-4a48-4ca1-9583-2bb4504ba84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AN\\anaconda3\\envs\\mamba\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool1d'>.\n",
      "MFLOPs: 23.18 M\n",
      "Parameters: 214.47 K\n",
      "Output Shape: torch.Size([1, 2])\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register count_softmax() for <class 'torch.nn.modules.activation.Softmax'>.\n",
      "[INFO] Register count_relu() for <class 'torch.nn.modules.activation.LeakyReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool1d'>.\n",
      "MFLOPs: 23.18 M\n",
      "Parameters: 214.47 K\n",
      "Output Shape: torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "from SMNet import LDMamba\n",
    "\n",
    "# Model initialization and profiling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL = LDMamba(in_channels=257, out_channels1=128, out_channels2=128, kernel_size1=5, kernel_size2=3, padding=2, stride=1, linear=64).to(device)\n",
    "\n",
    "# Generate a sample input tensor for profiling\n",
    "sample_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "flops, params = profile(MODEL, inputs=(sample_input,))\n",
    "print(f'MFLOPs: {flops / 1e6:.2f} M')\n",
    "print(f'Parameters: {params / 1e3:.2f} K')\n",
    "\n",
    "# Model inference\n",
    "output = MODEL(sample_input)\n",
    "print(f\"Output Shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc4493-5f36-4c11-9dae-f36f9f8a5c93",
   "metadata": {},
   "source": [
    "# 优化器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c4a227-1e8a-431d-9a69-e8bbfac4fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.AdamW(MODEL.parameters(), lr=LEARNING_RATE)\n",
    "Learn_rate_limit = LEARNING_RATE / 50\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=LEARNING_RATE, eta_min=Learn_rate_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc6383-092f-4990-9f20-f7583ed2ffec",
   "metadata": {},
   "source": [
    "# 训练模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc292a4e-2ff3-4685-90be-0cbf70f2c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_prefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_input, self.next_target = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_input = None\n",
    "            self.next_target = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            self.next_input = self.next_input.cuda(non_blocking=True)\n",
    "            self.next_target = self.next_target.cuda(non_blocking=True)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        signal, label = self.next_input, self.next_target\n",
    "        self.preload()\n",
    "        return signal, label\n",
    "\n",
    "def TRAIN_FUNCTION(function, optimizer, train):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    LOSS_FUNCTION = torch.nn.CrossEntropyLoss().to(device)\n",
    "    losses = []\n",
    "    perfetcher = data_prefetcher(train)\n",
    "    signal, label = perfetcher.next()\n",
    "    while signal is not None:\n",
    "        signal, label = signal.to(device), label.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "\n",
    "            output = function(signal)\n",
    "            loss = LOSS_FUNCTION(output, label.long())\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        signal, label = perfetcher.next()\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "def TEST_FUNCTION(function, test):\n",
    "    ACCH = []\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    for signal, label in test:\n",
    "        signal, label = signal.to(device), label.to(device)\n",
    "        output = function(signal)\n",
    "        ACCURACY = (output.argmax(1) == label).sum() / len(label)\n",
    "    ACCH.append(ACCURACY)\n",
    "    mean_accuracy = sum(ACCH) / len(ACCH)\n",
    "    return mean_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3632002-a8fb-4fb3-92ec-d65c8866aca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|\u001b[36m███████████████████████████████████████████\u001b[0m| , accuracy=0.8889, loss=0.01941\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "loop = tqdm(range(500), ncols=100, colour='cyan', desc='Training Progress', bar_format='{desc}: {percentage:3.0f}%|{bar}| {postfix}', leave = True)\n",
    "MAX_ACCURACY = []\n",
    "for _ in loop:\n",
    "    mean_loss = TRAIN_FUNCTION(MODEL, optimizer, train)\n",
    "    mean_accuracy = TEST_FUNCTION(MODEL, test)\n",
    "    MAX_ACCURACY.append(mean_accuracy)\n",
    "    loop.set_postfix(\n",
    "    loss = f\"{mean_loss:.5f}\",\n",
    "    accuracy =  f\"{max(MAX_ACCURACY).item():.4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
